{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d2c7f1be-5adc-497f-8a88-a63ce44d9838",
   "metadata": {},
   "source": [
    "# Logistic Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0523b0cb-53b5-4f65-8470-2b930cfc9a54",
   "metadata": {},
   "source": [
    "### Classification vs. Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e3c5702-dca8-417a-85a8-6fc3f4295a1b",
   "metadata": {},
   "source": [
    "---\n",
    "- In the previous unit, you learned about one of the supervised learning tasks called  linear regression where the goal was to predict a continuous real value (Price of house ) .\n",
    "  \n",
    "- In this unit, you will learn about another type of supervised learning task called classification.\n",
    "\n",
    "- In classification, we predict a discrete value .\n",
    "  \n",
    "-  We are actually predicting which class a sample belongs to based on its features.\n",
    "\n",
    "   For example, predicting if a person has a disease or not based on his health information .Here, having the disease can be      represented using a discrete value 1 and not having the disease can be represented using another discrete value of 0.\n",
    "\n",
    "Similarly, here are some other simple examples of the classification problem :\n",
    "\n",
    "  - **Predicting if an image is of a cat (0) or a dog (1),**  \n",
    "  - **Predicting if a car will get sold (1) or not (0) based on the features and details of car,**  \n",
    "  - **Classifying an email as spam (1) or not (0)**\n",
    "  \n",
    "#### Logistic regression is one of the basic classification algorithm.  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b7b0f4c-7d77-409e-90a6-e421906f09e4",
   "metadata": {},
   "source": [
    "To better understand classification problems and logistic regression, we will start with a simple example. The example contains the SAT score of the students as a feature $x$ and a corresponding label $y$ with values 1 or 0 indicating the admission of the particular student to a university.\n",
    " \n",
    "Here,\n",
    "  \n",
    "* 1 - The student gets admission\n",
    " \n",
    "* 0 - The student does not get admission.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a70efd0-66c8-4438-baac-0cbf1a7642f8",
   "metadata": {},
   "source": [
    "### Imports "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de8e5d84-f0ac-4920-bac8-d530f806cb9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import pandas as pd \n",
    "import numpy as np \n",
    "import matplotlib.pyplot as plt\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7387c21-a231-4ce3-9da2-6e94aac74e5c",
   "metadata": {},
   "source": [
    "### Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19490ce6-0966-4808-bc5e-74af94af054e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "data_path = \"https://drive.google.com/uc?export=download&id=1rLEMxfeaQc64RPiPDzXBy7ipI-R-kIlI\"\n",
    "\n",
    "#data_path= \"https://github.com/Aashishrimal/Logistic-Regression-/\"\n",
    "\n",
    "# Read the CSV data from the link\n",
    "data_frame = pd.read_csv(data_path)\n",
    "\n",
    "# Printfirst 5 samples from the DataFrame\n",
    "data_frame.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f1c033f-6ac0-4eb5-afa8-31a4c386253c",
   "metadata": {},
   "source": [
    "  \n",
    "Based on the dataset generated above, the task here is to predict if a student gets admission or not given their SAT Score. \n",
    "Initially lets plot the data ."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6b88249-83ba-4ac1-ab64-95fd1585b178",
   "metadata": {},
   "source": [
    "### Data visulization "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2022bd98-e9c6-4768-991f-1ef19a1491e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "map_color = np.array(['r', 'b'])\n",
    "\n",
    "x = data_frame.iloc[:, :-1].values\n",
    "y = data_frame.iloc[:, 1].values\n",
    "\n",
    "plt.scatter(data_frame['SAT Score'], data_frame['Admitted'], c=map_color[y])\n",
    "\n",
    "plt.title('SAT Score and Admission')\n",
    "plt.xlabel('SAT Score')\n",
    "plt.ylabel('Gets Admission - Yes or No')\n",
    "plt.savefig('sigmoid_data.png')\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ae4dc34-82ff-4007-860a-36ed9a37bd78",
   "metadata": {},
   "source": [
    "Initially lets see a function named Sigmoid Function.\n",
    "\n",
    "\n",
    "$$f(z) = \\frac 1 {(1+ e^{-z})} $$ $$    \n",
    "       = \\frac{e^z}{e^z + 1}  $$\n",
    "\n",
    "\n",
    " \n",
    "where $z$ is any input real number. We can plot $f(z)$ as follows:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e7a751c-6014-43b8-b616-73d90618a288",
   "metadata": {},
   "source": [
    "**Initially lets visualize a  function named Sigmoid function :**\n",
    "\n",
    "<a data-flickr-embed=\"true\" href=\"https://www.flickr.com/photos/200947226@N07/53845358796/in/dateposted-public/\" title=\"sigmoid\"><img src=\"https://live.staticflickr.com/65535/53845358796_f8714a4857_b.jpg\" width=\"1024\" height=\"307\" alt=\"sigmoid\"/></a><script async src=\"//embedr.flickr.com/assets/client-code.js\" charset=\"utf-8\"></script>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46304d6b-5d3f-43a1-bee2-1db2e8ead4fb",
   "metadata": {},
   "source": [
    "**Can you imagine fitting this graph  into the data mentioned in figure .?**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8875c5f2-9f5f-4927-bdb3-e1dca15b75fd",
   "metadata": {},
   "source": [
    "<div style=\"display: flex; justify-content: space-between;\">\n",
    "  <div style=\"margin-right: 10px;\">\n",
    "    <a data-flickr-embed=\"true\" href=\"https://www.flickr.com/photos/200947226@N07/53847194057/in/dateposted-public/\" title=\"sigmoid_data\">\n",
    "      <img src=\"https://live.staticflickr.com/65535/53847194057_fd1cf44f2e_z.jpg\" width=\"320\" height=\"240\" alt=\"sigmoid_data\"/>\n",
    "    </a>\n",
    "    <script async src=\"//embedr.flickr.com/assets/client-code.js\" charset=\"utf-8\"></script>\n",
    "  </div>\n",
    "  <div>\n",
    "    <a data-flickr-embed=\"true\" href=\"https://www.flickr.com/photos/200947226@N07/53845358796/in/dateposted-public/\" title=\"sigmoid\">\n",
    "      <img src=\"https://live.staticflickr.com/65535/53845358796_f8714a4857_b.jpg\" width=\"790\" height=\"240\" alt=\"sigmoid\"/>\n",
    "    </a>\n",
    "    <script async src=\"//embedr.flickr.com/assets/client-code.js\" charset=\"utf-8\"></script>\n",
    "  </div>\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ca01e3d-4824-44a5-be9c-31f11bc667a9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "1419d433-4d04-4900-b8cf-29f3c4ccc168",
   "metadata": {},
   "source": [
    "\n",
    "#### We can make  change in value of w and b so as to best fit the sigmoid curve into the data.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53cf87b6-f376-45c1-b44f-dd6bf59744b6",
   "metadata": {},
   "source": [
    "**Lets make some changes in the sigmoid function.**\n",
    "\n",
    "\n",
    "****$$f(z) = \\frac 1 {(1+ e^{-z})} $$ $$    \n",
    "       = \\frac{e^z}{e^z + 1}  $$****\n",
    "\n",
    "\n",
    "**On multiplying z by m**\n",
    "\n",
    "\n",
    "****$$f(z) = \\frac 1 {(1+ e^{-mz})} $$ $$    \n",
    "       = \\frac{e^{mz}}{e^{mz} + 1}  $$****\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf50f349-8923-45a5-83e3-27bad50d105f",
   "metadata": {},
   "source": [
    "**Lets visualize the change in the graph on multiplying  z by m .**  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e776172a-09e1-4c5f-9da3-b1c472c3bb01",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "remove_input"
    ]
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Sigmoid function\n",
    "def sigmoid(z):\n",
    "    return 1 / (1 + np.exp(-z))\n",
    "\n",
    "# x range centered at transition = 0\n",
    "x = np.linspace(-5, 5, 400)\n",
    "\n",
    "# Different w values to show effect\n",
    "w_values = [0.5, 1.0, 2.0, 5.0]\n",
    "colors = ['blue', 'green', 'orange', 'red']\n",
    "\n",
    "# Plot setup\n",
    "plt.figure(figsize=(12, 3))\n",
    "for w, color in zip(w_values, colors):\n",
    "    z = w * x  # b = 0\n",
    "    y = sigmoid(z)\n",
    "    plt.plot(x, y, label=f'w = {w}, b = 0', color=color)\n",
    "\n",
    "# Formatting\n",
    "plt.title(\"Effect of Different 'w' on Sigmoid Curve (Transition at x = 0)\")\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('Sigmoid(w * x)')\n",
    "plt.axvline(0, color='gray', linestyle='--', alpha=0.6)  # transition point\n",
    "plt.ylim(-0.05, 1.05)\n",
    "plt.grid(True)\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8ebbeaa-77a7-41ec-9b6e-bf6a6973482e",
   "metadata": {},
   "source": [
    "#### We can see ; increasing the value of m increase the slop in transition ."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad7e0c96-8680-462b-bdd2-d8ffee429bb1",
   "metadata": {},
   "source": [
    "***Now lets see what happens if we add b to w.z    \n",
    "i.e (w\\*z + b )***\n",
    "\n",
    "**On adding c to  w.z**\n",
    "\n",
    "\n",
    "****$$f(z) = \\frac 1 {(1+ e^{-(wz+b)})} $$ $$    \n",
    "       = \\frac{e^{wz+b}}{e^{wz+b} + 1}  $$****\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d1ba04d-de93-4e68-9539-222f672be32b",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "remove_input"
    ]
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Sigmoid function\n",
    "def sigmoid(z):\n",
    "    return 1 / (1 + np.exp(-z))\n",
    "\n",
    "# Fixed weight\n",
    "w = 3\n",
    "\n",
    "# Bias values to test\n",
    "bias_values = [-5, -2, 0, 2, 5]\n",
    "colors = ['blue', 'green', 'black', 'orange', 'red']\n",
    "\n",
    "# X range\n",
    "x = np.linspace(-5, 5, 400)\n",
    "\n",
    "# Plot setup\n",
    "plt.figure(figsize=(12, 3))\n",
    "for b, color in zip(bias_values, colors):\n",
    "    y = sigmoid(w * x + b)\n",
    "    plt.plot(x, y, label=f'w = {w}, b = {b}', color=color)\n",
    "\n",
    "# Add transition reference line\n",
    "plt.axvline(0, color='gray', linestyle='--', alpha=0.5)\n",
    "\n",
    "# Final formatting\n",
    "plt.title(\"Effect of Bias (b) on Sigmoid Curve (w fixed)\")\n",
    "plt.xlabel(\"x\")\n",
    "plt.ylabel(\"Sigmoid(w * x + b)\")\n",
    "plt.ylim(-0.05, 1.05)\n",
    "plt.grid(True)\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f42e77f1-bb33-4f99-8fad-e16bd7c6dcaa",
   "metadata": {},
   "source": [
    "#### Keypoint :  Changing the value of w shifts the logistic curve ."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eedc7cc8-a8ca-4a78-a2ba-775baff440de",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "- Now you might be clear regarding how the change is made in simplest form of sigmoid function to make best fit.  \n",
    "- But this work of finding the value of m and c to best fit the data is not performed by us ; it is the job of computer.  \n",
    "- Computer finds the best value of m and c  using gradient descent similar  to  the way it's done in linear Regression  .  \n",
    "- Instead of using SSE as loss in Linear Regression , we use Binary Cross Entroy Loss function .  \n",
    "\n",
    "****Well ,  \n",
    "What is Binary Cross Entropy Loss Function**** ? .  \n",
    "For each data point , \n",
    "\n",
    "****$$ \\mathcal{L}(y, \\hat{y}) = -\\left( y \\log(\\hat{y}) + (1 - y) \\log(1 - \\hat{y}) \\right) $$****\n",
    " where ,  \n",
    " $y$ is the true label (either 0 or 1),\n",
    "\n",
    "$\\hat{y}$ is the predicted probability (output of the logistic function).  \n",
    "\n",
    "\n",
    "      \n",
    "\n",
    "### Total Loss : \n",
    "****$$ J  = -\\frac{1}{m} \\sum_{i=1}^{m} \\left[ y^{(i)} \\log(\\hat{y}^{(i)}) + (1 - y^{(i)}) \\log(1 - \\hat{y}^{(i)}) \\right]$$****\n",
    "\n",
    "Where ,   \n",
    "*m is the number of training examples,*\n",
    "\n",
    "*$y^{(i)}$ is the true label for the i-th example*\n",
    "\n",
    "*$\\hat{y}^{(i)}$ \n",
    "  is the predicted probability for the i-th example.*\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1830e45-5906-4f9e-8dd9-312155a5d383",
   "metadata": {},
   "source": [
    "##### Now the value of m and c is obtained using `Gradient Descent` so as to minimize the value of Total Loss.\n",
    "###### Refer to Chapter - Simple Linear Regression in this Course to get insight regarding Gradient Descent \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7de6f25-6c3b-4da6-9910-5dacc8a1e18c",
   "metadata": {},
   "source": [
    "### Change the values of w and b to make the graph best fit the data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3a8579a-f1f5-4ef0-950f-167bf84cf92e",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "remove_output"
    ]
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import plotly.graph_objects as go\n",
    "import ipywidgets as widgets\n",
    "from IPython.display import display\n",
    "\n",
    "# --- Mouse Obesity Dataset ---\n",
    "np.random.seed(42)\n",
    "transition = 0.035\n",
    "n_per_class = 20\n",
    "\n",
    "non_obese_weights = np.random.uniform(0.025, 0.034, n_per_class - 1)\n",
    "non_obese_labels = np.zeros(n_per_class - 1)\n",
    "\n",
    "obese_weights = np.random.uniform(0.036, 0.045, n_per_class - 1)\n",
    "obese_labels = np.ones(n_per_class - 1)\n",
    "\n",
    "outlier_obese = np.random.uniform(0.034, 0.035, 1)\n",
    "outlier_obese_labels = np.ones(1)\n",
    "\n",
    "outlier_non_obese = np.random.uniform(0.035, 0.036, 1)\n",
    "outlier_non_obese_labels = np.zeros(1)\n",
    "\n",
    "weights = np.concatenate([\n",
    "    non_obese_weights,\n",
    "    outlier_obese,\n",
    "    obese_weights,\n",
    "    outlier_non_obese\n",
    "])\n",
    "labels = np.concatenate([\n",
    "    non_obese_labels,\n",
    "    outlier_obese_labels,\n",
    "    obese_labels,\n",
    "    outlier_non_obese_labels\n",
    "])\n",
    "\n",
    "sort_idx = np.argsort(weights)\n",
    "weights = weights[sort_idx]\n",
    "labels = labels[sort_idx]\n",
    "\n",
    "# --- Optimized Logistic Functions ---\n",
    "def safe_sigmoid(z):\n",
    "    z = np.clip(z, -500, 500)\n",
    "    return 1 / (1 + np.exp(-z))\n",
    "\n",
    "# --- Newton's Method for Optimal Fit ---\n",
    "def newton_method(X, y):\n",
    "    X = np.column_stack([np.ones(X.shape[0]), X])\n",
    "    beta = np.zeros(X.shape[1])\n",
    "    for _ in range(10):\n",
    "        p = safe_sigmoid(X @ beta)\n",
    "        p = np.clip(p, 1e-15, 1-1e-15)\n",
    "        W = np.diag(p * (1 - p))\n",
    "        gradient = X.T @ (y - p)\n",
    "        try:\n",
    "            hessian = X.T @ W @ X\n",
    "            delta = np.linalg.solve(hessian, gradient)\n",
    "            beta += delta\n",
    "            if np.linalg.norm(delta) < 1e-6:\n",
    "                break\n",
    "        except np.linalg.LinAlgError:\n",
    "            break\n",
    "    return beta[1], beta[0]\n",
    "\n",
    "w_optimal, b_optimal = newton_method(weights.reshape(-1, 1), labels)\n",
    "\n",
    "\n",
    "m = w_optimal * (-0.001)\n",
    "b = b_optimal * 1.5  \n",
    "\n",
    "# --- Create Interactive Figure ---\n",
    "fig = go.FigureWidget()\n",
    "\n",
    "# Add data points\n",
    "fig.add_scatter(\n",
    "    x=weights, y=labels, mode='markers',\n",
    "    marker=dict(size=8, color='blue'),\n",
    "    showlegend=False\n",
    ")\n",
    "\n",
    "# Create adaptive x-values with ultra-high density in transition region\n",
    "transition_point = -b/m if m != 0 else 0\n",
    "x_vals = np.concatenate([\n",
    "    np.linspace(-100, transition_point - 0.1, 3000),\n",
    "    np.linspace(transition_point - 0.1, transition_point + 0.1, 10000),  # Ultra-high density\n",
    "    np.linspace(transition_point + 0.1, 100, 3000)\n",
    "])\n",
    "\n",
    "# Add perfectly smooth curve with spline interpolation\n",
    "fig.add_scatter(\n",
    "    x=x_vals,\n",
    "    y=safe_sigmoid(m * x_vals + b),\n",
    "    mode='lines',\n",
    "    line=dict(\n",
    "        color='red', \n",
    "        width=3,\n",
    "        shape='spline',  # Natural curve smoothing\n",
    "        smoothing=1.3    # Optimal smoothness\n",
    "    ),\n",
    "    showlegend=False\n",
    ")\n",
    "\n",
    "# Decision boundary\n",
    "fig.add_vline(\n",
    "    x=transition_point,\n",
    "    line=dict(color=\"green\", dash=\"dash\", width=1.5),\n",
    "    annotation_text=f\"Decision Boundary: {transition_point:.5f} kg\",\n",
    "    annotation_position=\"top right\"\n",
    ")\n",
    "\n",
    "# Layout configuration\n",
    "fig.update_layout(\n",
    "    title='Interactive Logistic Regression',\n",
    "    xaxis_title='Weight (kg)',\n",
    "    yaxis_title='Obesity Probability',\n",
    "    xaxis=dict(\n",
    "        range=[0.02, 0.05],  # Initial view matches original\n",
    "        autorange=False,\n",
    "        fixedrange=False  # Enable pan/zoom\n",
    "    ),\n",
    "    yaxis=dict(\n",
    "        range=[-0.1, 1.1],\n",
    "        fixedrange=True\n",
    "    ),\n",
    "    height=400,\n",
    "    dragmode='pan',\n",
    "    margin=dict(l=50, r=50, b=50, t=50)\n",
    ")\n",
    "\n",
    "# --- Interactive Controls ---\n",
    "inc_m = widgets.Button(description='↑ m ', layout={'width': '100px'})\n",
    "dec_m = widgets.Button(description='↓ m ', layout={'width': '100px'})\n",
    "inc_b = widgets.Button(description='↑ b ', layout={'width': '100px'})\n",
    "dec_b = widgets.Button(description='↓ b ', layout={'width': '100px'})\n",
    "\n",
    "m_label = widgets.Label(value=f\"Slope (m): {m:.2f}\")\n",
    "b_label = widgets.Label(value=f\"Bias (b): {b:.2f}\")\n",
    "\n",
    "def update_plot():\n",
    "    transition_point = -b/m if m != 0 else 0\n",
    "    # Regenerate x-values to follow new transition point\n",
    "    new_x = np.concatenate([\n",
    "        np.linspace(-100, transition_point - 0.1, 3000),\n",
    "        np.linspace(transition_point - 0.1, transition_point + 0.1, 10000),\n",
    "        np.linspace(transition_point + 0.1, 100, 3000)\n",
    "    ])\n",
    "    \n",
    "    with fig.batch_update():\n",
    "        fig.data[1].x = new_x\n",
    "        fig.data[1].y = safe_sigmoid(m * new_x + b)\n",
    "        fig.layout.shapes[0].x0 = transition_point\n",
    "        fig.layout.shapes[0].x1 = transition_point\n",
    "        fig.layout.annotations[0].update(\n",
    "            x=transition_point,\n",
    "            text=f\"Decision Boundary: {transition_point:.5f} kg\"\n",
    "        )\n",
    "    \n",
    "    m_label.value = f\"Slope (m): {m:.2f}\"\n",
    "    b_label.value = f\"Bias (b): {b:.2f}\"\n",
    "\n",
    "# Button handlers\n",
    "inc_m.on_click(lambda _: (globals().update(m=m+200), update_plot()))\n",
    "dec_m.on_click(lambda _: (globals().update(m=max(1, m-200)), update_plot()))\n",
    "inc_b.on_click(lambda _: (globals().update(b=b+5), update_plot()))\n",
    "dec_b.on_click(lambda _: (globals().update(b=b-5), update_plot()))\n",
    "\n",
    "# Control panel\n",
    "controls = widgets.VBox([\n",
    "    widgets.HBox([dec_m, inc_m, m_label]),\n",
    "    widgets.HBox([dec_b, inc_b, b_label]),\n",
    "    widgets.Label(\"← → to pan | Scroll to zoom\", \n",
    "                style={'font-style': 'italic', 'color': 'gray'})\n",
    "])\n",
    "\n",
    "# Display\n",
    "display(controls)\n",
    "display(fig)\n",
    "\n",
    "# Print reference values\n",
    "print(f\"Optimal parameters: w = {w_optimal:.2f}, b = {b_optimal:.2f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b65c7ef-d104-4c14-abc5-a8cc28705c76",
   "metadata": {},
   "source": [
    "#### Note :\n",
    "##### The 'w' determines the slope of transition portion of graph .   \n",
    "##### The combined effect of 'w' and 'b' makes change in shifting of the graph . "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eafccac9-a810-4dfd-8b96-c2944e0027a1",
   "metadata": {},
   "source": [
    "## Next Chapter on Multiple Features , Multiple classes "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
